<!DOCTYPE html>
<html lang="en">
<meta charset="utf-8"/>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="description" content="Pedro O S">
  <meta name="keywords" content="mathematics programming">
  <meta name="author" content="Pedro O S">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>A survey of some Big Data tools - Pedro Sobota</title>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-33675243-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-33675243-1');
</script>

<script>
MathJax = {
  loader: {load: ['[tex]/tagformat']},
  // startup: {
  //   pageReady: () => {
  //     return MathJax.startup.defaultPageReady();
  //   }
  // },
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']], 
    packages: {'[+]': ['tagformat']},
    // color: {
    //   padding: '5px'
    // }
    //tags: 'ams',
    // tagformat: {
    //   // number: (n) => n.toString(),
    //   tag:    (tag) => 
    //     //'((' + tag + '))',
    //     'Formula(' + tag + ')',
    //   // id:     (id) => 'mjx-eqn-' + id.replace(/\s/g, '_'),
    //   // url:    (id, base) => base + '#' + encodeURIComponent(id),
    // }
  }, 
  chtml: {
    scale: 1
  }
}
</script>

<!-- <script src='img/MathJax-2.7.7/MathJax.js?config=TeX-MML-AM_CHTML,local/local.js'></script> -->
<script src='img/MathJax-3.1.2-custom/tex-mml-chtml.js'></script>

<!--<link rel="stylesheet" type="text/css" href="useit.css"/>-->
<link rel="stylesheet" type="text/css" href="img/style_header.css"/>
<link rel="stylesheet" type="text/css" href="img/style_body_article.css"/>

<!-- <link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Yantramanav:wght@300&display=swap" rel="stylesheet"> -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet"><link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@500&family=Source+Code+Pro:wght@500&display=swap" rel="stylesheet">

<link rel="stylesheet" type="text/css" href="img/lab/style_body_article_lab.css"/>

</head>

<body class="lab">

<header>



<!--%%toc%%-->

</header>

<nav>

<div style="float: left; width: 10px; margin-right: 20px;">
</div>
<div style="float: left;">
  <ul>
    <li><a href="index.html">Home</a></li>
    <li><a href="toc.html">Mathematics</a></li>
    <li><a href="labtoc.html">Programming</a></li>
    <li><a href="bgdtoc.html">Big Data</a></li>
    <!-- <li><a href="bustoc.html">Business</a></li> -->
    <!-- <li><a href="gdpr.html">Laws</a></li> -->
    <!-- <li><a href="cv.html">CV</a></li>
    <li><a href="contact.html">Contact</a></li> -->
    <!-- <li><a href="about.html">About</a></li> -->
  </ul>
</div>

</nav>

<div class="breadcrumb"></div>

<div class="content">
<h1>A survey of some Big Data tools</h1>
<div class="blogtags"><span class="blogtag bigdata">Big Data</span></div>

<div class="article_details">
<p>Pedro Sobota</p><hr/>
</div>

<p>In this post, we'll review some Big Data tools and go into some aspects of their usage as well as some technical details. We'll begin with a popular message broker: Apache Kafka.</p>

<p><b>Apache Kafka</b>. Apache Kafka (usable as a Docker image) is a high-capacity, high-throughput open-source distributed messaging service. Its API contains Admin (for interacting with the broker), Producer (for sending messages) and Consumer (for consuming messages). Contrary to a common client-server model, in Kafka the Consumer is more of a passive receiver in that its main job is to supply parameters to the broker and then fetch messages in a loop. As such, despite its many commands, it doesn't really directly control the message exchange, but rather acts more like a processor which may preserve messages for storage or transform them and include them in a landing area.</p>
<p>Kafka can receive messages from many sources (with each source being able to define a topic) and send these messages out to different combinations of Consumers, in different combinations of partitions. Its consuming behavior may also be automatic with respect to partitions, in that it may send from whichever partitions the broker considers appropriate for the currently connected Consumer, for the selected topics, or the partitions may be manually specified. In automatic mode, it acts as a load balancer that interacts with "signals" from the Consumer(s) which determine the best distribution and volume of messages to send to each Consumer. The Broker actively listens for metrics from Consumers to determine how to best interact with them. Each change in the distribution from the Broker's side is denominated a <i>rebalance</i>. Consumers are able to interact with rebalances to take proper action, avoiding data loss and failing gracefully, if needed.</p>
<p>Kafka is also flexible in how offsets are committed. From fully automatic to fully manual, a variety of options are available; the more manual control tends to yield the greatest robustness. Tuning configuration parameters is also necessary to inform the Broker of longer processing times on the Consumer without the Consumer being disconnected.</p>
<p>Due to multi-partition fetching, depending on message count and size limits and the consumption rate, Kafka can send messages at a very high rate. This places the emphasis for large ingestions on the receiving site (a landing site, for example), and its ability to insert concommitant to the consumption rate. While not trivial to configure for initial consumption, this variability in parameters allow for the scenario of reingestion.</p>

<p><b>Apache Spark</b>. Apache Spark (downloadable through the project's website) is an open-source general big data processing framework with libraries for machine learning, graph processing, and more. Spark has a low-level mode (RDD, Resilient Distributed Dataset) and a higher-level mode (Spark SQL) built on top of the RDD mode. Essentially, Apache SQL provides abstractions for more semantic operations on data over what RDD offers, such that both code can be more expressive and the engine can better optimize. Simply speaking, Spark (RDD or SQL) creates a four-step execution plan for a given operation (parsed logical, analyzed logical, optimized logical and physical) with the aim to produce the best sequence of procedures (strictly speaking, it's a DAG - Directed Acyclic Graph) the cluster, under command of the Driver node, should execute to process the inputs efficiently, over potentially large inputs and complex sets of transformations. The plan generator also takes additional parameters like cluster configuration, user configuration and query hints to generate the plan.</p>
<p>Spark SQL offers a full set of SQL operations and many more operations like <code>collect_set</code> and <code>collect_list</code>, for collecting elements into array values, JSON operations like extracting JSON values as strings and parsing JSON, and Struct querying with the syntax <code>a.b.c (...)</code>. Amongst its data types, Spark SQL supports Structs, hierarchical structures of arbitrary and array data types that can be mapped to and from JSON and be assigned as data types in columns in data frames. The Data Frame feature set also includes typed Data Frames in the form of Data Sets (Data Frames where the type of the element is a class other than Row); through Encoders, rows can be easily converted into objects (and Data Frames into Data Sets). Additional features also allow sending variables to Executor nodes (broadcasts) and memory or disk caching.</p>
<p>Spark is a multi-source and destination tool: sources such as Kafka, CSV, JSON, Parquet, text can be easily consumed while text and database destinations are easily available (CSV, Parquet, and multiple databases through the JDBC interface). Noteworthy is the <b>Kafka</b> source: through it, you can consume topics directly into a Data Frame and, optionally, specify partitions and offsets using a JSON string. Also, through the <b>Hive</b> integration, tables can be accessed and created and read from/written to in the Hive catalog as Data Frames. More integration options are available through external projects.</p>
<p>Simply speaking, Spark's work is also divided in <i>partitions</i>. The overall target number of partitions can be controlled from a Data Frame or RDD; and the number of Executors in the Cluster will dictate how much parallel work the cluster can do based on that number. Spark features a built-in UI where the actual parallelization of the work can be inspected for troubleshooting or improvements. Spark can automatically partition work without explicit directives but the ideal partitioning will involve a well-defined partitioning key that minimizes <i>skew</i>. Not only is efficient partitioning a good practice on the data side but it's a cost-saver since the same job definitions will perform better and finish faster. The ideal partitioning and execution plan combination would allow a job to finish with each partition fully and permanently allocated in a single Executor &mdash; thus incurring zero <i>shuffles</i> as a result (a <i>shuffle</i> is a transmission of data between nodes in the cluster during the execution of a job). One reason for employing a RDD over a Data Frame is that the RDD offers the possibility of defining a custom partitioner, thus allowing more control on this front.</p>
<p>Spark is a multi-language framework and supports Scala, Python, R and Java. <i>Update: Spark 3.4 includes support for a new connection protocol which allows more programming languages and platforms to participate &mdash; see <a href="https://spark.apache.org/docs/3.4.0/spark-connect-overview.html">Spark Connect Overview</a></i>.</p>

<p><b>Databricks</b>. Databricks (freely accessible through its Community Edition) is a data engineering, ML and data science environment that integrates Apache Spark. It features a Notebook interface and ease of creation and manipulation of project resources (like clusters and files). Databricks also has an Enterprise version containing a range of more advanced features.</p>

<p><b>Azure Data Factory</b>. Azure Data Factory (accessible through the Azure Free Tier) is an orchestration service (ETL, ELT and data integration). The main interface of Data Factory consists of <i>pipelines</i> (with supporting configuration units &mdash; Linked Services, Triggers, Datasets) that are modular in nature and can be easily copied, changed, debugged, and executed: the pipelines are parameterized and contain <i>variables</i> and control flow structures like <i>for loops</i>, <i>branching</i> ("if conditions") and <i>subpipelines</i>. Pipelines can be run with particular sets of parameters and can be observed. Parallelization of work can be easily achieved without additional instructions by using a <i>for loop</i>, and concurrency can be achieved through split nodes. The projects and all supporting configuration units are defined as JSON sets and activities can be configured through JSON. Data Factory also supports many data-oriented activities and data integrations, including external data integrations.</p>

<p>With this, it's time to wrap up.</p>

<p>This was a review of some Big Data tools in the Big Data toolset. Hope you found useful and see you next time!</p>
</div>

<div class="breadcrumb"></div>

<footer>
Copyright &copy 2018-2023 Pedro Sobota. All rights reserved.
</footer>

</body>
</html>